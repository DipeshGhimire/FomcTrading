{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_mpwJszjA78"
      },
      "outputs": [],
      "source": [
        "#Import packages\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from scipy.stats import kruskal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting FOMC Data from Forbes [THIS WILL STOP WORKING IF THE WEBSITE CHANGES]\n",
        "\n",
        "#Uncomment to show the entire dataframe\n",
        "#pd.set_option('display.max_rows', None)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "\n",
        "def fomcData():\n",
        "    url = \"https://www.forbes.com/advisor/investing/fed-funds-rate-history/\"\n",
        "    agent = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "    response = requests.get(url, headers=agent)\n",
        "    html = response.text\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    tables = soup.find_all('table', class_='foo-table')\n",
        "\n",
        "    dataframes = []\n",
        "\n",
        "    for table in tables:\n",
        "        headers = [header.text for header in table.find_all(\"th\")]\n",
        "\n",
        "        rows = table.find_all(\"tr\")[1:]\n",
        "\n",
        "        table_data = []\n",
        "        for row in rows:\n",
        "            cells = row.find_all(\"td\")\n",
        "            row_data = []\n",
        "            for cell in cells:\n",
        "                text = cell.text.strip()\n",
        "                if '%' in text and 'to' in text:\n",
        "                    text = text.split('to')[-1].strip()\n",
        "                row_data.append(text)\n",
        "            table_data.append(row_data)\n",
        "\n",
        "        df = pd.DataFrame(table_data, columns=headers)\n",
        "        dataframes.append(df)\n",
        "\n",
        "    all_data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "    all_data['FOMC Meeting Date'] = all_data['FOMC Meeting Date'].str.replace('.', '', regex=True)\n",
        "    all_data['FOMC Meeting Date'] = pd.to_datetime(all_data['FOMC Meeting Date'], errors='coerce')\n",
        "\n",
        "    all_data.reset_index(drop=True, inplace=True)\n",
        "    all_data = all_data.set_index('FOMC Meeting Date')\n",
        "    all_data = all_data.sort_values(by='FOMC Meeting Date', ascending=True)\n",
        "\n",
        "    if 'Change (bps)' in all_data.columns:\n",
        "        mask = (all_data.index.year >= 1999) & (all_data.index.year <= 2001)\n",
        "        all_data.loc[mask, 'Rate Change (bps)'] = all_data.loc[mask, 'Change (bps)']\n",
        "        all_data = all_data.drop(columns=['Change (bps)'])\n",
        "\n",
        "    all_data['Rate Change (bps)'] = all_data['Rate Change (bps)'].str.replace('+', '', regex=True).astype(float).astype('Int32', errors='ignore')\n",
        "    all_data['Federal Funds Rate'] = all_data['Federal Funds Rate'].str.replace('%', '').astype(float) / 100\n",
        "\n",
        "    #Getting all the FOMC Data to fill in 'No Change' Data\n",
        "    allFOMC = pd.read_csv('fomc_dates.csv', index_col='Date')\n",
        "    allFOMC.index = pd.to_datetime(allFOMC.index, format='%Y%m%d', errors='coerce')\n",
        "\n",
        "    # Merge the DataFrames on their date index, using an outer join.\n",
        "    merged_data = allFOMC.merge(all_data, how='left', left_index=True, right_index=True)\n",
        "\n",
        "    # Fill NaN values in 'Rate Change (bps)' column with 0\n",
        "    merged_data['Rate Change (bps)'].fillna(0, inplace=True)\n",
        "\n",
        "    merged_data['Federal Funds Rate'].fillna(method='ffill', inplace=True)\n",
        "\n",
        "    # Set a tolerance of 3 days\n",
        "    tolerance = pd.Timedelta(days=3)\n",
        "\n",
        "    # Use merge_asof to merge with tolerance\n",
        "    merged_data = pd.merge_asof(allFOMC, all_data, left_index=True, right_index=True, direction='nearest', tolerance=tolerance)\n",
        "\n",
        "    # Fill NaN values in 'Rate Change (bps)' column with 0\n",
        "    merged_data['Rate Change (bps)'].fillna(0, inplace=True)\n",
        "    merged_data['Federal Funds Rate'].fillna(method='ffill', inplace=True)\n",
        "\n",
        "    start_date = pd.Timestamp('1990-11-13')\n",
        "    merged_data = merged_data[merged_data.index >= start_date]\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "fomcData()"
      ],
      "metadata": {
        "id": "4zkrfhIljCZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifying Data into 6 Quads\n",
        "def classificationData():\n",
        "    df = fomcData()\n",
        "\n",
        "    def classify_action(val):\n",
        "      if val > 0:\n",
        "          return 'Tightening'\n",
        "      elif val < 0:\n",
        "          return 'Easing'\n",
        "      else:\n",
        "          return 'No Change'\n",
        "\n",
        "    df['Action'] = df[\"Rate Change (bps)\"].apply(classify_action)\n",
        "\n",
        "    return df\n",
        "\n",
        "classificationData()"
      ],
      "metadata": {
        "id": "fZdUIlLal9kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fomc = classificationData()\n",
        "fomc.index.names = ['Date']\n",
        "fomc"
      ],
      "metadata": {
        "id": "Pnil33wqtuOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the bond data\n",
        "def getBond():\n",
        "    endDate = dt.datetime.now()\n",
        "    startDate = '1990-01-01'\n",
        "    interval = '1d'\n",
        "    bondData = yf.download('SHY',startDate ,endDate, interval=interval)\n",
        "    bondData['Bond Close'] = bondData['Adj Close']\n",
        "    bondData= bondData['Bond Close']\n",
        "    bondData.index.names = ['Date']\n",
        "    return bondData\n",
        "\n",
        "getBond()"
      ],
      "metadata": {
        "id": "f55vzkiBsJcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifying Bond Data on Whether Bonds went Up or Down day of FOMC announcement\n",
        "def bondUpDown():\n",
        "    bondData = getBond()\n",
        "    df = fomc.merge(bondData, left_index=True, right_index=True, how='outer')\n",
        "\n",
        "    missing_bond_data = df[df['Bond Close'].isna()]\n",
        "    for idx, row in missing_bond_data.iterrows():\n",
        "        next_idx = df.loc[idx:].dropna(subset=['Bond Close']).index.min()\n",
        "\n",
        "        if pd.notna(next_idx):\n",
        "            for col in fomc.columns:\n",
        "                df.at[next_idx, col] = row[col]\n",
        "\n",
        "    df = df.dropna(subset=['Bond Close'])\n",
        "    df['Bonds PA'] = df['Bond Close'].diff().apply(lambda x: 'Up' if x > 0 else 'Down')\n",
        "\n",
        "    return df\n",
        "\n",
        "df = bondUpDown()"
      ],
      "metadata": {
        "id": "hclf6E8gdC4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fdaab89-ca85-4b27-b1e8-4ee2526a2bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Asset = 'SPY'\n",
        "\n",
        "#Getting ETF Data\n",
        "def getAsset():\n",
        "    endDate = dt.datetime.now()\n",
        "    startDate = '1990-01-01'\n",
        "    interval = '1d'\n",
        "    assetData =  yf.download(Asset, startDate ,endDate, interval=interval)\n",
        "    assetData['Asset Close'] = assetData['Adj Close']\n",
        "    assetData= assetData['Asset Close']\n",
        "    assetData.index.names = ['Date']\n",
        "    return assetData\n",
        "\n",
        "getAsset()"
      ],
      "metadata": {
        "id": "P3ZOav6vFz9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifying Data based on Rolling Mean (Above or Below Mean)\n",
        "def assetRM():\n",
        "    assetData = getAsset()\n",
        "    data = df.merge(assetData, on=\"Date\")\n",
        "    data['Asset RM'] = data[\"Asset Close\"].rolling(20).mean()\n",
        "    data['Mean Check'] = ['Above' if close > rolling_mean else 'Below' for close, rolling_mean in zip(data['Asset Close'], data['Asset RM'])]\n",
        "    data['Sit'] =  data['Action'] + ' '  + data['Bonds PA'] + ' ' + data['Mean Check']\n",
        "    return data\n",
        "\n",
        "data = assetRM()"
      ],
      "metadata": {
        "id": "JeVPDHyQdX5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of rows for the in-sample and out-of-sample portions\n",
        "total_rows = len(data)\n",
        "in_sample_rows = int(0.7 * total_rows)\n",
        "out_of_sample_rows = total_rows - in_sample_rows\n",
        "\n",
        "# Split the DataFrame into in-sample and out-of-sample\n",
        "in_sample = data.iloc[:in_sample_rows]\n",
        "out_of_sample = data.iloc[in_sample_rows:]\n",
        "\n",
        "# Now, 'in_sample' contains the first 70% of the data, and 'out_of_sample' contains the last 30%.\n",
        "data = in_sample"
      ],
      "metadata": {
        "id": "FVOVsbJd2wIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the code for optimizing a particular scenario (DO NOT USE WHILE USING THE OUT-OF-SAMPLE DATASET)\n",
        "def scenarioOptimzer():\n",
        "    best_sharpe = -float('inf')\n",
        "    best_entryDate = None\n",
        "    best_exitDate = None\n",
        "    best_positive_sharpe = -float('inf')\n",
        "    best_negative_sharpe = float('inf')\n",
        "\n",
        "    for entryDate in range(1, 10):\n",
        "        for exitDate in range(2, 11):\n",
        "            current_entryDate = entryDate\n",
        "            current_exitDate = exitDate\n",
        "\n",
        "            mask = (data['fomc'] == 1.0) & (~data['fomc'].isna())\n",
        "\n",
        "            # Convert date-based entryDate and exitDate to row-based periods\n",
        "            data['entryDate'] = data['Asset Close'].shift(-1 * current_entryDate)\n",
        "            data['exitDate'] = data['Asset Close'].shift(-1 * current_exitDate)\n",
        "\n",
        "            data['percent_change'] = (data['exitDate'] / data['entryDate'] - 1) * 100\n",
        "            data['percent_change'] = data['percent_change'].where(mask)\n",
        "            fomc_data = data[data['fomc'] == 1.0]\n",
        "            num_days_per_meeting = current_exitDate - current_entryDate\n",
        "\n",
        "            # Multiply by 8 (the average number of times the FOMC meets per year) for scaling\n",
        "            scale = num_days_per_meeting * 8\n",
        "\n",
        "            des = fomc_data.groupby(['Sit'])['percent_change'].describe()\n",
        "\n",
        "            # Handle cases where standard deviation is zero or negative\n",
        "            des['std'] = np.maximum(des['std'], 1e-6)  # Set a small positive minimum value for std\n",
        "            des['Sharpe'] = (des['mean'] / des['std']) * np.sqrt(scale)\n",
        "\n",
        "            final = des\n",
        "            final.index.name = 'Sit'  # Set the index name to 'Sit'\n",
        "\n",
        "            # Extract the Sharpe ratio for the specific group you're interested in\n",
        "            current_sharpe = final.loc['Tightening Up Above'][\"Sharpe\"]\n",
        "\n",
        "            if abs(current_sharpe) > best_sharpe:\n",
        "                best_sharpe = abs(current_sharpe)\n",
        "                best_entryDate = current_entryDate\n",
        "                best_exitDate = current_exitDate\n",
        "\n",
        "            if current_sharpe > best_positive_sharpe:\n",
        "                best_positive_sharpe = current_sharpe\n",
        "\n",
        "            if current_sharpe < best_negative_sharpe:\n",
        "                best_negative_sharpe = current_sharpe\n",
        "\n",
        "    return best_entryDate, best_exitDate, best_sharpe, best_positive_sharpe, best_negative_sharpe\n",
        "\n",
        "\n",
        "''' UNCOMMENT THIS TO RUN THE OPTIMZER FOR A PARTICULAR SCENARIO\n",
        "best_entryDate, best_exitDate, best_sharpe, best_positive_sharpe, best_negative_sharpe = scenarioOptimzer()\n",
        "\n",
        "print(\"Best Entry Date:\", best_entryDate)\n",
        "print(\"Best Exit Date:\", best_exitDate)\n",
        "print(\"Best Absolute Sharpe Ratio:\", best_sharpe)\n",
        "print(\"Best Positive Sharpe Ratio:\", best_positive_sharpe)\n",
        "print(\"Best Negative Sharpe Ratio:\", best_negative_sharpe)\n",
        "'''"
      ],
      "metadata": {
        "id": "YeSwD8DJhXFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the Sharpe based on a particular entry and exit date (Use Entry and Exit dates based on scenarioOptimzer() for a particular scenario)\n",
        "def calculateReturn(entryDate, exitDate):\n",
        "    mask = (data['fomc'] == 1.0) & (~data['fomc'].isna())\n",
        "\n",
        "    data['entryDate'] = data['Asset Close'].shift(-1 * entryDate)\n",
        "    data['exitDate'] = data['Asset Close'].shift(-1 * exitDate)\n",
        "\n",
        "    data['percent_change'] = (data['exitDate'] / data['entryDate'] - 1) * 100\n",
        "    data['percent_change'] = data['percent_change'].where(mask)\n",
        "    fomc_data = data[data['fomc'] == 1.0]\n",
        "    num_days_per_meeting = exitDate - entryDate\n",
        "\n",
        "    # multiply by 8 (the average number of times the FOMC meets per year) for scaling\n",
        "    scale = num_days_per_meeting * 8\n",
        "\n",
        "    des = fomc_data.groupby(['Sit'])['percent_change'].describe()\n",
        "    des['Sharpe'] = (des['mean']/des['std']) * np.sqrt(scale)\n",
        "\n",
        "    final = des\n",
        "    final['Sit'] = des.index\n",
        "    return final"
      ],
      "metadata": {
        "id": "MKDDFT1qlU30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Outputting the Sharpe for a particular Entry and Exit Date\n",
        "des = calculateReturn(1,9)\n",
        "des"
      ],
      "metadata": {
        "id": "Upstp-6pu3GN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
